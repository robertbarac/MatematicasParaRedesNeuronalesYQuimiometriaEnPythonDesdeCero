{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal Múltiple\n",
    "\n",
    "Ya teniendo clara la regresión lineal simple **sin importar si el método es por descenso del gradiente o por mínimos cuadrados con resolución de ecuaciones lineales con dos incógnitas**, es fácil comprender la múltiple. En la simple hay $m$ ejemplos de una sola característica de entrada, en la múltiple hay $m$ conjuntos de ejemplos de dos o más características. Cada característica tendrá su respectiva derivada parcial.\n",
    "\n",
    "La regresión lineal múltiple se entiende de la siguiente forma:\n",
    "$$\\begin{equation}\n",
    "f_{\\vec{w},b}(\\vec{x}) = w_{1}*x_{1} + w_{2}*x_{2} + ... + w_{n}*x_{n} + b\n",
    "\\end{equation}\n",
    "$$\n",
    "teniendo en cuenta que $\\vec{w}$ es un vector de pesos, cuya longitud es igual a la de cualquier vector de ejemplo. El vector de pesos sería: $\\vec{w} = [w_{1}, w_{2}, ..., w_{n}]$ y sobre un ejemplo x es un vector: $\\vec{x} = [x_{1}, x_{2}, ..., x_{n}]$\n",
    "que pertenece a una matriz X:\n",
    "$X_{m,n} = \\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & ... & x_{1n}\\\\\n",
    "x_{21} & x_{22} & ... & x_{2n}\\\\\n",
    ". & . & . & .\\\\\n",
    ". & . & x_{ij} & .\\\\\n",
    ". & . & . & .\\\\\n",
    "x_{m1} & x_{m2} & ... & x_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$\n",
    "\n",
    "La matriz de ejemplos tendrá una dimensión de $(m,n)$, que al ser hecho el producto punto por el vector $\\vec{w}$ este vector debe ser de dimensión $(n,1)$ y el resultado, que será de $(m,1)$, se les debe sumar el escalar $b$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera que la derivada respecto a $b$ sigue siendo como la de una regresión simple, pero la derivada respecto a w sería:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{J_{\\vec{w}, b}}(\\vec{w}, b)}{\\partial{w}} = -\\frac{2}{m}\\sum_{i=1}^{m}(y - wx -b)*x_{j}^{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "donde i es el vector ejemplo i-ésimo y j es el dato en la posición j-ésima del vector $x^{i}$, es decir, $x_{j}^{i}$ representa un escalar, y tiene todo el sentido porque vamos a intentar calcular la derivada.\n",
    "\n",
    "La derivada respecto al sesgo $b$ es:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{J_{\\vec{w}, b}}(\\vec{w}, b)}{\\partial{b}} = -\\frac{2}{m}\\sum_{i=1}^{m}(y - wx -b)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos numpy para mejorar los tiempos de cálculo de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costo a este w : 1.5578904045996674e-12\n"
     ]
    }
   ],
   "source": [
    "def predecir(x, w, b):\n",
    "    prediccion = np.dot(x, w) + b\n",
    "    return prediccion\n",
    "\n",
    "def costo(X, y, w, b): \n",
    "    m = X.shape[0]\n",
    "    costo = 0.0\n",
    "    for i in range(m):                                \n",
    "        y_hat = np.dot(X[i], w) + b           \n",
    "        costo = costo + (y[i] - y_hat)**2      \n",
    "    costo = costo / (2*m)                         \n",
    "    return costo\n",
    "\n",
    "cost = costo(X_train, y_train, w_init, b_init)\n",
    "print(f'Costo a este w : {cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_gradiente(x, y, w, b):\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(x[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * x[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regresion_multiple(x, y, w_in, b_in, costo, gradiente, alpha, epocas):\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in) \n",
    "    b = b_in\n",
    "    for i in range(epocas):\n",
    "        dj_db, dj_dw = gradiente(x, y, w, b)\n",
    "        # actualizamos pesos y el sesgo\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        if i < 100000:\n",
    "            J_history.append(costo(x, y, w, b))\n",
    "        if i % math.ceil(epocas/10) == 0:\n",
    "            print(f\"Iteración {i:4d}: Costo {J_history[-1]:8.2f}\")\n",
    "    return w, b, J_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración    0: Costo  2529.46\n",
      "Iteración  100: Costo   695.99\n",
      "Iteración  200: Costo   694.92\n",
      "Iteración  300: Costo   693.86\n",
      "Iteración  400: Costo   692.81\n",
      "Iteración  500: Costo   691.77\n",
      "Iteración  600: Costo   690.73\n",
      "Iteración  700: Costo   689.71\n",
      "Iteración  800: Costo   688.70\n",
      "Iteración  900: Costo   687.69\n",
      "b,w found by gradient descent: -0.00,[ 0.20396569  0.00374919 -0.0112487  -0.0658614 ] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = regresion_multiple(X_train, y_train, initial_w, initial_b, costo, calcular_gradiente, alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora miremos la otra forma, la matricial para resolver para muchas incógnitas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forma matricial\n",
    "\n",
    "Si tenemos la forma de nuestra función lineal múltiple y la volvemos en forma matricial: $y = X_{(m,n)}w_{(n,1)} + e_{(m,1)}$.\n",
    "\n",
    "Recordemos que: $X_{m,n} = \\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & ... & x_{1n}\\\\\n",
    "x_{21} & x_{22} & ... & x_{2n}\\\\\n",
    ". & . & . & .\\\\\n",
    ". & . & x_{ij} & .\\\\\n",
    ". & . & . & .\\\\\n",
    "x_{m1} & x_{m2} & ... & x_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$\n",
    "pero como hay que colocar el $w_0$(este es el sesgo o intercepto $b$ que hemos utilizado anteriormente) en el vector $w$ y sería de dimensión $(n+1,1)$, colocamos una columna de 1's a la izquierda, así:\n",
    "$X_{m,n+1} = \\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & ... & x_{1n}\\\\\n",
    "1 & x_{21} & x_{22} & ... & x_{2n}\\\\\n",
    ". & . & . & . & .\\\\\n",
    ". & . & . & x_{ij} & .\\\\\n",
    ". & . & . & . & .\\\\\n",
    "1 & x_{m1} & x_{m2} & ... & x_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$\n",
    "por lo que se convertiría en una matriz $(m,n+1)$\n",
    "\n",
    "se puede demostrar que minimizando la suma de los errores se puede proveer las ecuaciones normales: $X^{T}Xw = X^{T}y$,\n",
    "\n",
    "donde $\\begin{equation}w = \\begin{bmatrix} w_0\\\\ w_{1}\\\\ .\\\\ .\\\\ .\\\\ w_{m} \\end{bmatrix} = (X^{T}X)^{-1}X^{T}y \\end{equation}$\n",
    "\n",
    "$\\begin{equation}\n",
    "\\hat{y} = \\begin{bmatrix}\n",
    "\\hat{y_{1}}\\\\\n",
    "\\hat{y_{2}}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\hat{y_{n}}\n",
    "\\end{bmatrix} = Xw = X(X^{T}X)^{-1}X^{T}y\n",
    "\\end{equation}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo.\n",
    "Tenemos la siguiente tabla:\n",
    "| i  | x1(Rate) | x2(Temperature) | x3(Acid concentration) | y (Stack loss) |\n",
    "|----|----------|-----------------|------------------------|----------------|\n",
    "| 1  | 80       | 27              | 88                     | 37             |\n",
    "| 2  | 62       | 22              | 87                     | 18             |\n",
    "| 3  | 62       | 23              | 87                     | 18             |\n",
    "| 4  | 62       | 24              | 93                     | 19             |\n",
    "| 5  | 62       | 24              | 93                     | 20             |\n",
    "| 6  | 58       | 23              | 87                     | 15             |\n",
    "| 7  | 58       | 18              | 80                     | 14             |\n",
    "| 8  | 58       | 18              | 89                     | 14             |\n",
    "| 9  | 58       | 17              | 88                     | 13             |\n",
    "| 10 | 58       | 18              | 82                     | 11             |\n",
    "| 11 | 58       | 19              | 93                     | 12             |\n",
    "| 12 | 50       | 18              | 89                     | 8              |\n",
    "| 13 | 50       | 18              | 86                     | 7              |\n",
    "| 14 | 50       | 19              | 72                     | 8              |\n",
    "| 15 | 50       | 19              | 79                     | 8              |\n",
    "| 16 | 50       | 20              | 80                     | 9              |\n",
    "| 17 | 50       | 20              | 82                     | 15             |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la matriz $x$ y el vector $y$, llenando de 1's la primera columna de $x$:\n",
    "\n",
    "$X = \\begin{bmatrix}\n",
    "1 & 80 & 27 & 88\\\\\n",
    "1 & 62 & 27 & 87\\\\\n",
    ". & . & . & .\\\\\n",
    ". & . & . & .\\\\\n",
    ". & . & . & .\\\\\n",
    "1 & 50 & 20 & 80\\\\\n",
    "1 & 56 & 20 & 82\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "calculamos la operación por su traspuesta, dado que $X$ es dimensión $(m,n+1)$ y $X^{T}$ es dimensión $(n+1,m)$\n",
    "\n",
    "$y = \\begin{bmatrix}\n",
    "\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4], [5, 6, 7]]\n",
      "[[1, 2, 3, 4], [1, 5, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "def ones(matrix: List) -> List:\n",
    "    for fila in matrix:\n",
    "        fila.insert(0, 1)\n",
    "\n",
    "matriz = [[2,3,4],[5,6,7]]\n",
    "print(matriz)\n",
    "ones(matriz)\n",
    "print(matriz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ipynb.fs.full.C9_Vectores_y_Matrices import dimension, mult_matrices, transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.C9_Vectores_y_Matrices import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[80,27,88],[62, 22, 87],[62, 23, 87],\n",
    "    [62, 24, 93],[62, 24, 93],[58, 23, 87],\n",
    "    [58, 18, 80],[58, 18, 89],[58, 17, 88],\n",
    "    [58, 18, 82],[58, 19, 93],[50, 18, 89],\n",
    "    [50, 18, 86],[50, 19, 72],[50, 19, 79],\n",
    "    [50, 20, 80],[60, 20, 82]\n",
    "    ]\n",
    "\n",
    "y = [37, 18, 18, 19, 20, 15, 14, 14, \n",
    "    13, 11, 12, 8, 7, 8, 8, 9, 15]\n",
    "#print(ones(X), transpose(ones(X)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llenamos a $X$ de 1's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 80, 27, 88],\n",
       " [1, 62, 22, 87],\n",
       " [1, 62, 23, 87],\n",
       " [1, 62, 24, 93],\n",
       " [1, 62, 24, 93],\n",
       " [1, 58, 23, 87],\n",
       " [1, 58, 18, 80],\n",
       " [1, 58, 18, 89],\n",
       " [1, 58, 17, 88],\n",
       " [1, 58, 18, 82],\n",
       " [1, 58, 19, 93],\n",
       " [1, 50, 18, 89],\n",
       " [1, 50, 18, 86],\n",
       " [1, 50, 19, 72],\n",
       " [1, 50, 19, 79],\n",
       " [1, 50, 20, 80],\n",
       " [1, 60, 20, 82]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones(X)\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos $X^{T}X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz m2 traspuesta: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [80, 62, 62, 62, 62, 58, 58, 58, 58, 58, 58, 50, 50, 50, 50, 50, 60], [27, 22, 23, 24, 24, 23, 18, 18, 17, 18, 19, 18, 18, 19, 19, 20, 20], [88, 87, 87, 93, 93, 87, 80, 89, 88, 82, 93, 89, 86, 72, 79, 80, 82]]\n",
      "[[17, 986, 347, 1455], [986, 58060, 20380, 84682], [347, 20380, 7215, 29796], [1455, 84682, 29796, 125053]]\n"
     ]
    }
   ],
   "source": [
    "X_traspuesta = transpose(X)\n",
    "Xt_X = mult_matrices(X_traspuesta, X)\n",
    "print(Xt_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le calculamos la inversa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.42822245e+01, -6.13827645e-03, -2.89447806e-02,\n",
       "        -1.55121400e-01],\n",
       "       [-6.13827645e-03,  2.78637048e-03, -4.87789686e-03,\n",
       "        -6.53182397e-04],\n",
       "       [-2.89447806e-02, -4.87789686e-03,  1.73005133e-02,\n",
       "        -4.82206554e-04],\n",
       "       [-1.55121400e-01, -6.53182397e-04, -4.82206554e-04,\n",
       "         2.37005313e-03]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inversa = np.linalg.inv(np.array(Xt_X))\n",
    "inversa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora haremos $X^{T}y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [80, 62, 62, 62, 62, 58, 58, 58, 58, 58, 58, 50, 50, 50, 50, 50, 60],\n",
       " [27, 22, 23, 24, 24, 23, 18, 18, 17, 18, 19, 18, 18, 19, 19, 20, 20],\n",
       " [88, 87, 87, 93, 93, 87, 80, 89, 88, 82, 93, 89, 86, 72, 79, 80, 82]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_traspuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_matriz_vector(m1, v):\n",
    "    filas = len(m1)\n",
    "    salida = []\n",
    "    for i in range(filas):\n",
    "        salida[i].append(dot(m1[i], v))\n",
    "    return salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  246 15092  5295 21320]\n"
     ]
    }
   ],
   "source": [
    "Xt_y = np.dot(X_traspuesta, y)\n",
    "print(Xt_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finalmente hallemos el vector $w$, multiplicando Xt_y por la inversa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-39.66250309,   0.78757477,   0.58793868,  -0.04144391])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(inversa, Xt_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos valores son los de $b_0$, $b_1$ y sucesivamente, de manera que la ecuación es:\n",
    "\n",
    "$$\\hat{y} = -39.662 + 0.787x_1 + 0.587x_2 - 0.041x_3$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos con la librería de Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(np.array(X), np.array(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Éste es el $R^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787484853667733"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.78757477,  0.58793868, -0.04144391])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-39.6625030916248"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.57075852, 18.49616322, 19.0841019 , 19.42337711, 19.42337711,\n",
       "       15.93380283, 13.2842168 , 12.91122161, 12.36472684, 13.20132898,\n",
       "       13.33338464,  6.61062347,  6.7349552 ,  7.90310863,  7.61300125,\n",
       "        8.15949602, 15.95235588])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = reg.predict(X)\n",
    "y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos calcular el $R^{2}$ así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787484853667733"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y, y_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y el error cuadrático medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.020366841005721"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
